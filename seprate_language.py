#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°† LLM4RE_v4 æ ¼å¼çš„ JSON æ•°æ®é›†æŒ‰è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰åˆ†ç¦»
"""

import argparse
import json
from pathlib import Path
from tqdm import tqdm
from langdetect import detect, LangDetectException


def load_json_or_jsonl(path: str):
    """åŠ è½½JSONæˆ–JSONLæ–‡ä»¶"""
    p = Path(path)
    if p.suffix.lower() == ".jsonl":
        with p.open("r", encoding="utf-8") as f:
            return [json.loads(line) for line in f if line.strip()]
    else:
        with p.open("r", encoding="utf-8") as f:
            return json.load(f)


def write_jsonl(path: str, rows):
    """å†™å…¥JSONLæ–‡ä»¶"""
    with open(path, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False))
            f.write("\n")


def separate_by_language(data, text_key="sentence"):
    """
    æ ¹æ®è¯­è¨€åˆ†ç¦»æ•°æ®ã€‚

    Args:
        data (list): åŒ…å«æ•°æ®æ ·æœ¬çš„åˆ—è¡¨ã€‚
        text_key (str): åŒ…å«å¾…æ£€æµ‹æ–‡æœ¬çš„å­—æ®µåã€‚é»˜è®¤ä¸º "sentence"ã€‚

    Returns:
        tuple: åŒ…å«ä¸‰ä¸ªåˆ—è¡¨çš„å…ƒç»„ (chinese_data, english_data, other_data)
    """
    chinese_samples = []
    english_samples = []
    other_samples = []

    for item in tqdm(data, desc="Detecting language"):
        try:
            text_to_detect = item.get(text_key, "")
            if not text_to_detect or not isinstance(text_to_detect, str) or len(text_to_detect.strip()) == 0:
                # å¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ï¼Œå½’å…¥ other
                other_samples.append(item)
                continue

            # æ£€æµ‹è¯­è¨€
            detected_lang = detect(text_to_detect)

            # æ ¹æ® langdetect çš„è¿”å›å€¼è¿›è¡Œåˆ†ç±»
            if detected_lang.startswith('zh'): # 'zh', 'zh-cn', 'zh-hk', 'zh-tw', 'zh-yue' ç­‰
                chinese_samples.append(item)
            elif detected_lang == 'en':
                english_samples.append(item)
            else:
                other_samples.append(item)

        except LangDetectException:
            # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œå½’å…¥ other
            other_samples.append(item)
        except Exception as e:
            # å…¶ä»–æ½œåœ¨é”™è¯¯ï¼Œä¹Ÿå½’å…¥ other
            print(f"Warning: Error processing item: {item.get(text_key, '')[:50]}... Error: {e}")
            other_samples.append(item)

    return chinese_samples, english_samples, other_samples


def main():
    # è®¾ç½®é»˜è®¤è·¯å¾„
    default_input_path = "/home/users/lhy/LLM4RE_2Round/data/dev2.json"
    default_output_dir = "/home/users/lhy/LLM4RE_2Round/data" # é»˜è®¤è¾“å‡ºç›®å½•

    parser = argparse.ArgumentParser(description="å°† LLM4RE_v4 æ ¼å¼çš„ JSON æ•°æ®é›†æŒ‰è¯­è¨€ï¼ˆä¸­æ–‡/è‹±æ–‡ï¼‰åˆ†ç¦»")
    parser.add_argument("--input_path", type=str, default=default_input_path, help=f"è¾“å…¥æ•°æ®é›†è·¯å¾„ (JSON æˆ– JSONL), é»˜è®¤: {default_input_path}")
    parser.add_argument("--output_dir", type=str, default=default_output_dir, help=f"è¾“å‡ºç›®å½•è·¯å¾„, é»˜è®¤: {default_output_dir}")
    parser.add_argument("--text_key", type=str, default="sentence", help="åŒ…å«å¾…æ£€æµ‹æ–‡æœ¬çš„å­—æ®µåï¼Œé»˜è®¤ä¸º 'sentence'")

    args = parser.parse_args()

    input_path = Path(args.input_path)
    output_dir = Path(args.output_dir)

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not input_path.exists():
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return

    print(f"ğŸ“‚ åŠ è½½æ•°æ®: {input_path}")
    try:
        data = load_json_or_jsonl(str(input_path))
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼: {e}")
        return
    except Exception as e:
        print(f"é”™è¯¯: è¯»å–è¾“å…¥æ–‡ä»¶æ—¶å‡ºç°é—®é¢˜: {e}")
        return

    print(f"ğŸ” å¼€å§‹æŒ‰è¯­è¨€åˆ†ç¦»æ•°æ® (æ–‡æœ¬å­—æ®µ: '{args.text_key}')...")
    chinese_data, english_data, other_data = separate_by_language(
        data, text_key=args.text_key
    )

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)

    # å®šä¹‰è¾“å‡ºæ–‡ä»¶å
    base_name = input_path.stem
    zh_output_path = output_dir / f"{base_name}_zh.jsonl"
    en_output_path = output_dir / f"{base_name}_en.jsonl"
    other_output_path = output_dir / f"{base_name}_other.jsonl"

    # å†™å…¥æ–‡ä»¶
    print(f"ğŸ“ ä¿å­˜ä¸­æ–‡æ•°æ®åˆ°: {zh_output_path}")
    write_jsonl(str(zh_output_path), chinese_data)

    print(f"ğŸ“ ä¿å­˜è‹±æ–‡æ•°æ®åˆ°: {en_output_path}")
    write_jsonl(str(en_output_path), english_data)

    if other_data: # å¦‚æœ other_data ä¸ä¸ºç©º
        print(f"ğŸ“ ä¿å­˜å…¶ä»–è¯­è¨€/æ— æ³•è¯†åˆ«æ•°æ®åˆ°: {other_output_path}")
        write_jsonl(str(other_output_path), other_data)

    # è¾“å‡ºç»Ÿè®¡
    print("\nğŸ“Š åˆ†ç¦»ç»“æœç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(data)}")
    print(f"  ä¸­æ–‡æ ·æœ¬æ•°: {len(chinese_data)}")
    print(f"  è‹±æ–‡æ ·æœ¬æ•°: {len(english_data)}")
    print(f"  å…¶ä»–/æ— æ³•è¯†åˆ«æ ·æœ¬æ•°: {len(other_data)}")
    print(f"âœ… æ•°æ®åˆ†ç¦»å®Œæˆï¼")


if __name__ == "__main__":
    main()