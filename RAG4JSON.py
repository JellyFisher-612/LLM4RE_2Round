import json
import os
import argparse
from tqdm import tqdm
from collections import Counter
from typing import List, Dict, Any
from rag_utils import Retriever, detect_language, separate_by_language

def load_json_or_jsonl(path: str) -> List[Dict[Any, Any]]:
    """åŠ è½½ JSON æˆ– JSONL æ–‡ä»¶"""
    assert os.path.exists(path), f"File not found: {path}"
    if path.endswith(".json"):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    elif path.endswith(".jsonl"):
        with open(path, "r", encoding="utf-8") as f:
            data = [json.loads(line) for line in f]
    else:
        raise ValueError(f"Unsupported file type: {path}")
    print(f"åŠ è½½å®Œæˆ {path}, æ ·æœ¬æ•°: {len(data)}")
    return data

def filter_empty_outputs(samples: List[Dict[Any, Any]], key: str = "output") -> List[Dict[Any, Any]]:
    """è¿‡æ»¤æ‰æŒ‡å®šå­—æ®µä¸ºç©ºåˆ—è¡¨çš„æ ·æœ¬"""
    before = len(samples)
    filtered = [s for s in samples if not (isinstance(s.get(key), list) and len(s.get(key)) == 0)]
    after = len(filtered)
    print(f"   ğŸ” å·²è¿‡æ»¤æ‰ {before - after} æ¡ {key} ä¸ºç©ºçš„æ ·æœ¬ï¼Œå‰©ä½™ {after} æ¡ã€‚")
    return filtered

def main():
    parser = argparse.ArgumentParser(description="åŸºäºå‘é‡æ£€ç´¢çš„æ ·æœ¬å¢å¼º (ä»…éœ€åˆå¹¶çŸ¥è¯†åº“)")
    parser.add_argument("--knowledge_base_path", type=str, required=True, help="åˆå¹¶åçš„ä¸­è‹±æ–‡çŸ¥è¯†åº“è·¯å¾„ (.json/.jsonl)")
    parser.add_argument("--data_path", type=str, required=True, help="å¾…å¢å¼ºæ•°æ®è·¯å¾„ (.json/.jsonl)")
    parser.add_argument("--output_path", type=str, required=True, help="å¢å¼ºç»“æœè¾“å‡ºè·¯å¾„ (.json)")
    parser.add_argument("--text_key", type=str, default="input", help="ç”¨äºæ£€ç´¢çš„æ–‡æœ¬å­—æ®µ")
    parser.add_argument("--similarity_threshold", type=float, default=0.5, help="ç›¸ä¼¼åº¦é˜ˆå€¼")
    args = parser.parse_args()

    print(f"\nğŸ“š åŠ è½½åˆå¹¶çŸ¥è¯†åº“: {args.knowledge_base_path}")
    combined_kb = load_json_or_jsonl(args.knowledge_base_path)
    print(f"   æ ·æœ¬æ€»æ•°: {len(combined_kb)}")

    print(f"ğŸ”€ æŒ‰è¯­è¨€åˆ†ç¦»çŸ¥è¯†åº“ (å­—æ®µ: '{args.text_key}')...")
    kb_samples_zh, kb_samples_en, kb_samples_other = separate_by_language(combined_kb, text_key=args.text_key)

    # âœ… æ–°å¢ï¼šè¿‡æ»¤ output ä¸ºç©ºçš„æ ·æœ¬
    kb_samples_zh = filter_empty_outputs(kb_samples_zh, key="output")
    kb_samples_en = filter_empty_outputs(kb_samples_en, key="output")

    # === æ„å»º Retriever ===
    print("\nğŸš€ æ„å»ºå‘é‡ç´¢å¼•...")
    retriever_zh = Retriever(kb_samples_zh, key=args.text_key)
    retriever_en = Retriever(kb_samples_en, key=args.text_key)
    print(f"   âœ… ä¸­æ–‡ç´¢å¼•æ„å»ºå®Œæˆ ({len(kb_samples_zh)} æ¡æ ·æœ¬)")
    print(f"   âœ… è‹±æ–‡ç´¢å¼•æ„å»ºå®Œæˆ ({len(kb_samples_en)} æ¡æ ·æœ¬)")

    print(f"\nğŸ“‚ åŠ è½½å¾…å¢å¼ºæ ·æœ¬: {args.data_path}")
    samples = load_json_or_jsonl(args.data_path)
    print(f"   å¾…å¢å¼ºæ ·æœ¬æ•°: {len(samples)}")

    print(f"\nğŸ¯ å¼€å§‹æ£€ç´¢ç›¸ä¼¼æ ·æœ¬ (æ¯æ¡æ ·æœ¬é€‰å– 2 ä¸ª: ä¸€ä¸ª output ä¸ºç©ºï¼Œä¸€ä¸ª output éç©º)")
    augmented = []
    stats = Counter()

    for s in tqdm(samples, desc="Processing queries"):
        query = s.get(args.text_key, "")
        if not query.strip():
            stats["zero"] += 1
            continue

        detected_lang = detect_language(query)
        s["detected_language"] = detected_lang

        retriever = retriever_zh if detected_lang == 'zh' else retriever_en
        try:
            # å¤šå–ä¸€äº›ç»“æœï¼Œç”¨äºç­›é€‰
            examples, sims = retriever.retrieve(query=query, top_k=20, threshold=args.similarity_threshold)
        except Exception as e:
            print(f"âš ï¸ æ£€ç´¢å¤±è´¥: {query[:50]}... Error: {e}")
            stats["zero"] += 1
            continue

        if not examples:
            stats["zero"] += 1
            continue

        # æ–°çš„ç­›é€‰é€»è¾‘ï¼šé€‰ 1 ä¸ª output éç©º + 1 ä¸ª output ä¸ºç©º
        selected_examples = []
        selected_sims = []
        has_empty = False
        has_nonempty = False

        for ex, sim in zip(examples, sims):
            ex_text = ex.get(args.text_key, "").strip()
            ex_output = ex.get("output", [])

            # æ’é™¤è‡ªèº«
            if sim > 0.95 or ex_text == query.strip():
                continue

            # ä¼˜å…ˆé€‰ä¸€ä¸ª output éç©ºï¼Œä¸€ä¸ª output ç©º
            if not has_nonempty and ex_output:
                selected_examples.append(ex)
                selected_sims.append(sim)
                has_nonempty = True
            elif not has_empty and (isinstance(ex_output, list) and len(ex_output) == 0):
                selected_examples.append(ex)
                selected_sims.append(sim)
                has_empty = True

            # ä¸¤ç±»éƒ½æ‰¾åˆ°äº†å°±åœæ­¢
            if has_nonempty and has_empty:
                break

        if not selected_examples:
            stats["zero"] += 1
            continue

        s["similar_samples"] = selected_examples
        s["similarity_scores"] = selected_sims

        # ç»Ÿè®¡ä¿¡æ¯
        if has_nonempty and has_empty:
            stats["full"] += 1
        else:
            stats["partial"] += 1

        augmented.append(s)

    # === è¾“å‡ºç»Ÿè®¡ ===
    total = len(samples)
    print(f"\nğŸ” æ£€ç´¢ç»Ÿè®¡ï¼š")
    print(f"  å®Œæ•´ç»“æœ (æ‰¾åˆ°ç©º+éç©ºæ ·æœ¬): {stats['full']}")
    print(f"  éƒ¨åˆ†ç»“æœ: {stats['partial']}")
    print(f"  é›¶ç»“æœ: {stats['zero']}")
    print(f"  è¦†ç›–ç‡: {(1 - stats['zero']/total)*100:.1f}%")

    # === ä¿å­˜ç»“æœ ===
    with open(args.output_path, "w", encoding="utf-8") as f:
        json.dump(augmented, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ å¢å¼ºç»“æœå·²ä¿å­˜åˆ°: {args.output_path}")


if __name__ == "__main__":
    main()

# python rag_enhance.py \
#   --knowledge_base_path /root/autodl-tmp/LLM4RE_2Round/data/train2.json \
#   --data_path /root/autodl-tmp/LLM4RE_2Round/data/train2.json \
#   --output_path /home/users/lhy/LLM4RE_2Round/data/rag_train2.json \
#   --k 2 \
#   --similarity_threshold 0.6